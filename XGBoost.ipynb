{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.2.0-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\natalie davis\\anaconda3\\lib\\site-packages (from lightgbm) (1.4.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\natalie davis\\anaconda3\\lib\\site-packages (from lightgbm) (0.34.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\natalie davis\\anaconda3\\lib\\site-packages (from lightgbm) (0.22.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natalie davis\\anaconda3\\lib\\site-packages (from lightgbm) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\natalie davis\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.14.1)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import mode\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, roc_auc_score, confusion_matrix\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "np.random.seed(1729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTPATH = r'C:\\\\Users\\Natalie Davis\\Documents\\BIPOC_Kaggle\\tabular-playground-series-mar-2021\\\\'\n",
    "DATAPATH = r'C:\\\\Users\\Natalie Davis\\Documents\\BIPOC_Kaggle\\tabular-playground-series-mar-2021\\\\'\n",
    "\n",
    "TRAIN   = pd.read_csv(DATAPATH+'train.csv')\n",
    "TEST    = pd.read_csv(DATAPATH+'test.csv')\n",
    "cat_cols = [f for f in TRAIN.columns if 'cat' in f]\n",
    "num_cols = [f for f in TRAIN.columns if 'cont' in f]\n",
    "ID = 'id'\n",
    "TARGET = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_dev(datafile):\n",
    "    mu  = data.feature_value.mean()\n",
    "    std = np.sqrt(np.mean( (datafile.feature_value - mu)**2 ) )\n",
    "    return std\n",
    "\n",
    "def id_dup_cols(datafile):\n",
    "    remove = []\n",
    "    cols = datafile.columns.tolist()\n",
    "    for c in range(len(cols)-1):\n",
    "        v = datafile[cols[c]].values\n",
    "        for j in range(c+1,len(cols)):\n",
    "            if np.array_equal(v,datafile[cols[j]].values):\n",
    "                remove = remove + [cols[j], cols[c]]\n",
    "    remove = pd.DataFrame(remove, columns=['feature','is_dup_of'])    \n",
    "    return remove\n",
    "\n",
    "def feature_stats(datafile):\n",
    "    out=[]\n",
    "    features = list( set(datafile.columns.tolist()) - set([ID,TARGET]) )\n",
    "    for f in features:\n",
    "        feature_type    = datafile[f].dtype\n",
    "        trn_nonNA_rows  = datafile[(pd.isnull(datafile[f])==False) & (pd.isnull(datafile[TARGET])==False)].shape[0]\n",
    "        tst_nonNA_rows  = datafile[(pd.isnull(datafile[f])==False) & (pd.isnull(datafile[TARGET])==True)].shape[0]\n",
    "        level_count     = len(datafile[f].value_counts())\n",
    "        if feature_type == np.object:\n",
    "            datafile[f] = datafile[f].factorize(sort=True)[0]\n",
    "        min_value       = datafile[f].min()\n",
    "        per_10          = datafile[f].quantile(.10)\n",
    "        per_25          = datafile[f].quantile(.25)\n",
    "        med_value       = datafile[f].median()\n",
    "        mean_value      = datafile[f].mean()\n",
    "        mode_value      = mode(datafile[f])[0][0]\n",
    "        mode_count      = mode(datafile[f])[1][0]\n",
    "        per75           = datafile[f].quantile(.75)\n",
    "        per_90          = datafile[f].quantile(.90)\n",
    "        max_value       = datafile[f].max()\n",
    "        val_range       = max_value - min_value\n",
    "        val_stdev       = datafile[f].std()\n",
    "        mean_nonNA_Resp = datafile[TARGET][np.isnan(datafile[f])==False].mean()\n",
    "        corr            = datafile[[TARGET,f]].corr(method='pearson')\n",
    "        target_corr     = corr[TARGET][f]\n",
    "        \n",
    "        out = out + [(f,feature_type,trn_nonNA_rows,tst_nonNA_rows,level_count,min_value,per_10,per_25,med_value,mean_value,mode_value,\n",
    "                      per75,per_90,max_value,mode_count,val_range,val_stdev,mean_nonNA_Resp,target_corr)]\n",
    "    \n",
    "    outfile = pd.DataFrame(out,columns=['feature','feature_type','trn_nonNA_rows','tst_nonNA_rows','level_count',\\\n",
    "                                         'min_value','per_10','per_25','med_value','mean_value','mode_value','per75','per_90','max_value',\\\n",
    "                                         'mode_count','val_range','val_stdev','mean_nonNA_Resp','Response_corr'])\n",
    "    return outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated Rows:  0\n"
     ]
    }
   ],
   "source": [
    "data = TRAIN.append(TEST)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "feature_stats = feature_stats(data)\n",
    "dup_cols = id_dup_cols(data)\n",
    "feature_stats = pd.merge(feature_stats, dup_cols, on='feature', how='left')\n",
    "\n",
    "feature_stats.to_csv(DATAPATH+'Feature_Statistics.csv', index=False)\n",
    "cols = feature_stats.feature.tolist()\n",
    "data['dup_rows'] = data.duplicated(cols, keep=False)\n",
    "print ('Duplicated Rows: ', data['dup_rows'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_folds(trn, numfolds):\n",
    "    temp0 = trn[trn['target']==0]\n",
    "    temp0['fold'] = np.random.randint(numfolds, size=temp0.shape[0])\n",
    "    temp1 = trn[trn['target']==1]\n",
    "    temp1['fold'] = np.random.randint(numfolds, size=temp1.shape[0])\n",
    "    trn = temp0.append(temp1, ignore_index=True)\n",
    "    trn.sort_values(['id'], inplace=True)\n",
    "    trn.reset_index(drop=True, inplace=True)\n",
    "    return trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Encode Categorical features\n",
    "k_stat = 5\n",
    "f_stat = 20\n",
    "\n",
    "TRAIN = add_folds(TRAIN, 5)\n",
    "\n",
    "def get_prob_tgt(trn, tst, Id, feature, tgt, prior, k, f):\n",
    "    grouped = trn.groupby(feature, as_index=False)[tgt].agg({'size':np.size,'mean':np.mean})\n",
    "    grouped['lambda'] = grouped['size'].apply(lambda n: 1.0/(1 + np.exp( (k-n)/f )))\n",
    "    grouped[feature+'_tgtenc'] = (grouped['lambda'] * grouped['mean']) + ((1-grouped['lambda'])*prior)\n",
    "    tst = tst.merge(grouped, on=feature, how='left')\n",
    "    return tst[[Id, feature+'_tgtenc']]\n",
    "\n",
    "mean_y  = TRAIN['target'].mean()\n",
    "for col in cat_cols:\n",
    "    tr_values = pd.DataFrame()\n",
    "    for fld in range(5):\n",
    "        tr = TRAIN[TRAIN['fold'] != fld]\n",
    "        te = TRAIN[TRAIN['fold'] == fld]\n",
    "        lvl_prob  = get_prob_tgt(tr[['id', col, 'target']].copy(), te[['id', col]].copy(), 'id', col, 'target', mean_y, k_stat, f_stat)\n",
    "        tr_values = tr_values.append(lvl_prob)\n",
    "    te_values = get_prob_tgt(TRAIN[['id', col, 'target']].copy(), TEST[['id', col]].copy(), 'id', col, 'target', mean_y, k_stat, f_stat)\n",
    "    tr_values.replace([np.inf, -np.inf], mean_y, inplace=True)\n",
    "    tr_values.fillna(mean_y, inplace=True)\n",
    "    te_values.replace([np.inf, -np.inf], mean_y, inplace=True)\n",
    "    te_values.fillna(mean_y, inplace=True)\n",
    "    TRAIN = TRAIN.merge(tr_values, on=['id'], how='left')\n",
    "    TEST = TEST.merge(te_values, on=['id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[500]\tvalid_0's auc: 0.891882\n",
      "[1000]\tvalid_0's auc: 0.893744\n",
      "[1500]\tvalid_0's auc: 0.894168\n",
      "Early stopping, best iteration is:\n",
      "[1628]\tvalid_0's auc: 0.894284\n",
      "Fold Score:  0.8942838132795825\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[500]\tvalid_0's auc: 0.890483\n",
      "[1000]\tvalid_0's auc: 0.892237\n",
      "Early stopping, best iteration is:\n",
      "[1423]\tvalid_0's auc: 0.892896\n",
      "Fold Score:  0.892895708207069\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[500]\tvalid_0's auc: 0.890594\n",
      "[1000]\tvalid_0's auc: 0.892295\n",
      "[1500]\tvalid_0's auc: 0.892807\n",
      "Early stopping, best iteration is:\n",
      "[1516]\tvalid_0's auc: 0.892824\n",
      "Fold Score:  0.8928243790600016\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[500]\tvalid_0's auc: 0.890393\n",
      "[1000]\tvalid_0's auc: 0.89217\n",
      "[1500]\tvalid_0's auc: 0.892722\n",
      "Early stopping, best iteration is:\n",
      "[1738]\tvalid_0's auc: 0.892889\n",
      "Fold Score:  0.8928892674873078\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[500]\tvalid_0's auc: 0.890728\n",
      "[1000]\tvalid_0's auc: 0.892781\n",
      "[1500]\tvalid_0's auc: 0.893278\n",
      "Early stopping, best iteration is:\n",
      "[1566]\tvalid_0's auc: 0.893348\n",
      "Fold Score:  0.8933479089953154\n",
      "CV Score:  0.8932482154058552\n"
     ]
    }
   ],
   "source": [
    "#LightGBM Model\n",
    "param_dict = {}\n",
    "param_dict['boosting_type']    = 'gbdt'\n",
    "param_dict['objective']        = 'binary'\n",
    "param_dict['metric']           = 'auc'\n",
    "param_dict['num_leaves']       = 51\n",
    "param_dict['learning_rate']    = 0.02\n",
    "param_dict['colsample_bytree'] = 0.8\n",
    "param_dict['subsample']        = 0.6\n",
    "param_dict['max_depth']        = 7\n",
    "param_dict['subsample_freq']   = 1\n",
    "#param_dict['reg_alpha']        = 0.23\n",
    "#param_dict['reg_lambda']       = 0.16\n",
    "#param_dict['weight']           = 'name:wgt'\n",
    "param_dict['bagging_seed']     = 351\n",
    "param_dict['verbosity']        = -1\n",
    "MAX_ROUNDS = 30000\n",
    "STOP_ROUNDS = 50\n",
    "VERBOSE_EVAL = 500\n",
    "\n",
    "features = [f for f in TRAIN.columns if 'tgtenc' in f] + num_cols\n",
    "TRAIN = add_folds(TRAIN, 5)\n",
    "mean_score = 0.0\n",
    "test_preds = np.zeros(TEST.shape[0])\n",
    "for fold in range(5):\n",
    "    trn_X = np.array(TRAIN.loc[TRAIN['fold']!=fold, features])\n",
    "    trn_y = np.array(TRAIN.loc[TRAIN['fold']!=fold, 'target'])\n",
    "    val_X = np.array(TRAIN.loc[TRAIN['fold']==fold, features])\n",
    "    val_y = np.array(TRAIN.loc[TRAIN['fold']==fold, 'target'])\n",
    "    model = lgb.LGBMRegressor(**param_dict, n_estimators=MAX_ROUNDS, n_jobs=-1)\n",
    "    \n",
    "    model.fit(trn_X, trn_y, eval_set=(val_X, val_y), verbose=VERBOSE_EVAL, early_stopping_rounds=STOP_ROUNDS)\n",
    "    val_preds = model.predict(val_X)\n",
    "    score = roc_auc_score(val_y, val_preds)\n",
    "    mean_score += score/5\n",
    "    print('Fold Score: ', score)\n",
    "    \n",
    "    test_preds += model.predict(np.array(TEST[features]))/5\n",
    "    \n",
    "print('CV Score: ', mean_score)\n",
    "\n",
    "SUB = pd.DataFrame({'id':TEST['id'], 'target':test_preds})\n",
    "SUB.to_csv(ROOTPATH+'submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:07:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.86503\teval-auc:0.86204\n",
      "[500]\ttrain-auc:0.90109\teval-auc:0.89077\n",
      "[1000]\ttrain-auc:0.90919\teval-auc:0.89289\n",
      "[1500]\ttrain-auc:0.91466\teval-auc:0.89350\n",
      "[1679]\ttrain-auc:0.91631\teval-auc:0.89357\n",
      "Fold Score:  0.8935695306797514\n",
      "[12:17:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.86589\teval-auc:0.86168\n",
      "[500]\ttrain-auc:0.90172\teval-auc:0.89039\n",
      "[1000]\ttrain-auc:0.90956\teval-auc:0.89245\n",
      "[1500]\ttrain-auc:0.91498\teval-auc:0.89315\n",
      "[1617]\ttrain-auc:0.91602\teval-auc:0.89325\n",
      "Fold Score:  0.8932476137000606\n",
      "[12:27:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.86501\teval-auc:0.86420\n",
      "[500]\ttrain-auc:0.90117\teval-auc:0.89135\n",
      "[1000]\ttrain-auc:0.90936\teval-auc:0.89315\n",
      "[1500]\ttrain-auc:0.91497\teval-auc:0.89381\n",
      "[1581]\ttrain-auc:0.91577\teval-auc:0.89382\n",
      "Fold Score:  0.8938191445722392\n",
      "[12:36:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.86569\teval-auc:0.86409\n",
      "[500]\ttrain-auc:0.90120\teval-auc:0.89092\n",
      "[1000]\ttrain-auc:0.90933\teval-auc:0.89302\n",
      "[1500]\ttrain-auc:0.91474\teval-auc:0.89355\n"
     ]
    }
   ],
   "source": [
    "#XGBoost Model\n",
    "param_dict = {}\n",
    "param_dict['silent']           = True\n",
    "param_dict['objective']        = 'binary:logistic'\n",
    "param_dict['booster']          = 'gbtree'\n",
    "param_dict['eval_metric']      = 'auc'\n",
    "#param_dict['tree_method']      = 'exact'\n",
    "param_dict['eta']              = 0.02\n",
    "param_dict['colsample_bytree'] = 0.8\n",
    "param_dict['subsample']        = 0.8\n",
    "param_dict['max_depth']        = 7\n",
    "param_dict['min_child_weight'] = 51\n",
    "# param_dict['gamma']            = 1.0\n",
    "# param_dict['lambda']           = 0.5\n",
    "# param_dict['alpha']            = 0.0\n",
    "param_dict['base_score']       = 0.26\n",
    "param_dict['seed']             = 1729\n",
    "MAX_ROUNDS = 30000\n",
    "STOP_ROUNDS = 50\n",
    "VERBOSE_EVAL = 500\n",
    "\n",
    "features = [f for f in TRAIN.columns if 'tgtenc' in f] + num_cols\n",
    "TRAIN = add_folds(TRAIN, 5)\n",
    "mean_score = 0.0\n",
    "test_preds = np.zeros(TEST.shape[0])\n",
    "d_test = xgb.DMatrix(TEST[features], missing=np.nan)\n",
    "for fold in range(5):\n",
    "    d_train = xgb.DMatrix(TRAIN.loc[TRAIN['fold']!=fold, features], label=TRAIN.loc[TRAIN['fold']!=fold, 'target'], missing=np.nan)\n",
    "    d_valid = xgb.DMatrix(TRAIN.loc[TRAIN['fold']==fold, features], label=TRAIN.loc[TRAIN['fold']==fold, 'target'], missing=np.nan)\n",
    "    val_y = np.array(TRAIN.loc[TRAIN['fold']==fold, 'target'])\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "    model = xgb.train(param_dict, d_train, MAX_ROUNDS, watchlist, early_stopping_rounds=STOP_ROUNDS,verbose_eval= VERBOSE_EVAL)\n",
    "    val_preds = model.predict(xgb.DMatrix(TRAIN.loc[TRAIN['fold']==fold, features], missing=np.nan))\n",
    "    score = roc_auc_score(val_y, val_preds)\n",
    "    mean_score += score/5\n",
    "    print('Fold Score: ', score)\n",
    "    \n",
    "    test_preds += model.predict(d_test)/5\n",
    "    \n",
    "print('CV Score: ', mean_score)\n",
    "\n",
    "SUB = pd.DataFrame({'id':TEST['id'], 'target':test_preds})\n",
    "SUB.to_csv(ROOTPATH+'submission2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
